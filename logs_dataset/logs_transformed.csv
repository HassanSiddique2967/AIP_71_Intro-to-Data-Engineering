timestamp,event_time,log_level,process_id,module,thread,line_number,tag,text,date
1757511342804,"2025-09-10T13:35:42,804",info,10153,org.apache.spark.metrics.source.StageSkewness,Thread-10,29,Observability,Skewness metric using Skewness Factor = 5,2025-09-10
1757511342805,"2025-09-10T13:35:42,805",info,10154,org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener,Thread-10,68,,PerformanceMetricsSource is initiated,2025-09-10
1757511342805,"2025-09-10T13:35:42,805",info,10154,com.amazonaws.services.glue.GlueContext,Thread-10,129,,ObservabilityMetrics configured and enabled,2025-09-10
1757511342834,"2025-09-10T13:35:42,833",info,10182,com.amazonaws.services.glue.util.Job$,Thread-10,94,,runId Method is Invoked,2025-09-10
1757511342834,"2025-09-10T13:35:42,834",info,10183,com.amazonaws.services.glue.util.Job$,Thread-10,103,,Job run ID under runId method is jr_abeec7a629dbdd0b2a85341d68fc44240c90423c19d39a89788cb1e38c7c826d,2025-09-10
1757511342867,"2025-09-10T13:35:42,867",info,10216,com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory,Thread-10,72,,"Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)",2025-09-10
1757511342868,"2025-09-10T13:35:42,868",info,10217,com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory,Thread-10,85,,Set initial getObject socket timeout to 2000 ms.,2025-09-10
1757511342870,"2025-09-10T13:35:42,870",info,10219,com.amazonaws.services.glue.util.FileListPersistence,Thread-10,37,,create FileListPersistence with conf: fs.s3.serverSideEncryption.kms.keyId: None,2025-09-10
1757511342872,"2025-09-10T13:35:42,872",info,10221,com.amazonaws.services.glue.utils.EndpointConfig$,Thread-10,36,,STAGE is prod,2025-09-10
1757511342873,"2025-09-10T13:35:42,873",info,10222,com.amazonaws.services.glue.utils.EndpointConfig$,Thread-10,90,,"Endpoints: {credentials_provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain, glue.endpoint=https://glue.us-east-1.amazonaws.com, lakeformation.endpoint=https://lakeformation.us-east-1.amazonaws.com, jes.endpoint=https://glue-jes-prod.us-east-1.amazonaws.com, region=us-east-1}",2025-09-10
1757511342901,"2025-09-10T13:35:42,900",info,10249,com.amazonaws.services.glue.util.AvroReaderUtil$,Thread-10,245,,Creating default Avro field parser for version 1.7.,2025-09-10
1757511343062,"2025-09-10T13:35:43,061",info,10410,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_0 stored as values in memory (estimated size 243.0 KiB, free 5.8 GiB)",2025-09-10
1757511343157,"2025-09-10T13:35:43,156",info,10505,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_0_piece0 stored as bytes in memory (estimated size 40.6 KiB, actual size: 40.6 KiB, free 5.8 GiB)",2025-09-10
1757511343159,"2025-09-10T13:35:43,159",info,10508,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_0_piece0 in memory on 172.39.49.172:37387 (size: 40.6 KiB, free: 5.8 GiB)",2025-09-10
1757511343165,"2025-09-10T13:35:43,165",info,10514,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 0 from broadcast at DynamoConnection.scala:55,2025-09-10
1757511343178,"2025-09-10T13:35:43,178",info,10527,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_1 stored as values in memory (estimated size 243.0 KiB, free 5.8 GiB)",2025-09-10
1757511343221,"2025-09-10T13:35:43,221",info,10570,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_1_piece0 stored as bytes in memory (estimated size 40.6 KiB, actual size: 40.6 KiB, free 5.8 GiB)",2025-09-10
1757511343231,"2025-09-10T13:35:43,230",info,10579,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_1_piece0 in memory on 172.39.49.172:37387 (size: 40.6 KiB, free: 5.8 GiB)",2025-09-10
1757511343234,"2025-09-10T13:35:43,234",info,10583,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 1 from broadcast at DynamoConnection.scala:55,2025-09-10
1757511343236,"2025-09-10T13:35:43,236",info,10585,com.amazonaws.services.glue.util.JobBookmark$,Thread-10,71,,"jobbookmark is not enabled, do not init AWSGlueJobBookMarkService",2025-09-10
1757511343246,"2025-09-10T13:35:43,246",info,10595,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_0_piece0 on 172.39.49.172:37387 in memory (size: 40.6 KiB, free: 5.8 GiB)",2025-09-10
1757511343258,"2025-09-10T13:35:43,257",info,10606,com.amazonaws.services.glue.GlueContext,Thread-10,917,,Glue secret manager integration: secretId is not provided.,2025-09-10
1757511343829,"2025-09-10T13:35:43,829",info,11178,com.amazonaws.services.glue.GlueContext,Thread-10,1046,,The DataSource in action : com.amazonaws.services.glue.HadoopDataSource,2025-09-10
1757511343846,"2025-09-10T13:35:43,845",trace,11194,com.amazonaws.services.glue.HadoopDataSource,Thread-10,552,,DataSource - compression type from source options used: none,2025-09-10
1757511343873,"2025-09-10T13:35:43,872",info,11221,com.amazonaws.services.glue.hadoop.PartitionFilesListerUsingBookmark,Thread-10,392,,"IncompletePartitionFilter(partitionCreationEpoch=0, incompletePartition=)",2025-09-10
1757511343873,"2025-09-10T13:35:43,873",info,11222,com.amazonaws.services.glue.hadoop.PartitionFilesListerUsingBookmark,Thread-10,393,,"newPartitionFilter(partitionCreationEpoch=0, oldPartitions=Set())",2025-09-10
1757511343873,"2025-09-10T13:35:43,873",info,11222,com.amazonaws.services.glue.hadoop.PartitionFilesListerUsingBookmark,Thread-10,394,,"UnprocessedPartitionFilter(partitionCreationEpoch=0, oldPartitions=Set())",2025-09-10
1757511343874,"2025-09-10T13:35:43,874",info,11223,com.amazonaws.services.glue.hadoop.PartitionFilesListerUsingBookmark,Thread-10,399,,"last job run range: low inconsistency range begin: 1970-01-01T00:00:00Z,",2025-09-10
1757511343929,"2025-09-10T13:35:43,929",info,11278,com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory,Thread-10,72,,"Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)",2025-09-10
1757511343930,"2025-09-10T13:35:43,930",info,11279,com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory,Thread-10,85,,Set initial getObject socket timeout to 2000 ms.,2025-09-10
1757511344477,"2025-09-10T13:35:44,476",info,11825,com.amazonaws.services.glue.hadoop.PartitionFilesListerUsingBookmark,ForkJoinPool-2-worker-1,422,,"After initial job bookmarks filter, processing 100.00% of 1 files in partition DynamicFramePartition(com.amazonaws.services.glue.DynamicRecord@a0139eb8,s3://data-engineering-task-bucket/ncr_ride_bookings.csv,0).",2025-09-10
1757511344479,"2025-09-10T13:35:44,478",trace,11827,com.amazonaws.services.glue.HadoopDataSource,Thread-10,373,,DataSource - grouping_none,2025-09-10
1757511344479,"2025-09-10T13:35:44,479",info,11828,com.amazonaws.services.glue.HadoopDataSource,Thread-10,569,,"nonSplittable: false, disableSplitting: false, catalogCompressionNotSplittable: false, groupFilesTapeOption: none, format: csv, isColumnar: false",2025-09-10
1757511344516,"2025-09-10T13:35:44,515",info,11864,com.hadoop.compression.lzo.GPLNativeCodeLoader,Thread-10,34,,Loaded native gpl library,2025-09-10
1757511344520,"2025-09-10T13:35:44,520",info,11869,com.hadoop.compression.lzo.LzoCodec,Thread-10,76,,Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8],2025-09-10
1757511344658,"2025-09-10T13:35:44,657",info,12006,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_2 stored as values in memory (estimated size 245.7 KiB, free 5.8 GiB)",2025-09-10
1757511344669,"2025-09-10T13:35:44,669",info,12018,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_1_piece0 on 172.39.49.172:37387 in memory (size: 40.6 KiB, free: 5.8 GiB)",2025-09-10
1757511344693,"2025-09-10T13:35:44,692",info,12041,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_2_piece0 stored as bytes in memory (estimated size 41.8 KiB, actual size: 41.8 KiB, free 5.8 GiB)",2025-09-10
1757511344694,"2025-09-10T13:35:44,693",info,12042,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_2_piece0 in memory on 172.39.49.172:37387 (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511344699,"2025-09-10T13:35:44,699",info,12048,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 2 from newAPIHadoopRDD at DataSource.scala:440,2025-09-10
1757511344756,"2025-09-10T13:35:44,756",trace,12105,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511344863,"2025-09-10T13:35:44,862",trace,12211,com.amazonaws.services.glue.DynamicFrame,Thread-10,1469,,DynamicFrame - applyMapping,2025-09-10
1757511345033,"2025-09-10T13:35:45,032",trace,12381,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511345049,"2025-09-10T13:35:45,048",trace,12397,com.amazonaws.services.glue.DynamicFrame,Thread-10,363,,DynamicFrame - toDF,2025-09-10
1757511345208,"2025-09-10T13:35:45,208",info,12557,org.apache.spark.sql.internal.SharedState,Thread-10,60,,"spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.",2025-09-10
1757511345212,"2025-09-10T13:35:45,212",info,12561,org.apache.spark.sql.internal.SharedState,Thread-10,60,,Warehouse path is 'file:/tmp/spark-warehouse'.,2025-09-10
1757511348620,"2025-09-10T13:35:48,619",info,15968,org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator,Thread-10,60,,Code generated in 350.186655 ms,2025-09-10
1757511348677,"2025-09-10T13:35:48,676",trace,16025,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511348696,"2025-09-10T13:35:48,695",trace,16044,com.amazonaws.services.glue.DynamicFrame,Thread-10,363,,DynamicFrame - toDF,2025-09-10
1757511348872,"2025-09-10T13:35:48,871",info,16220,com.amazonaws.services.dataquality.EvaluateDataQuality$,Thread-10,210,,Starting Evaluate Data Quality Run jr_abeec7a629dbdd0b2a85341d68fc44240c90423c19d39a89788cb1e38c7c826d for job: Uber_ride_feature_engieering,2025-09-10
1757511349050,"2025-09-10T13:35:49,049",info,16398,com.amazonaws.glue.ml.dataquality.dqdl.model.condition.number.NumberBasedCondition,Thread-10,48,,Evaluating condition for rule: ColumnCount > 0,2025-09-10
1757511349050,"2025-09-10T13:35:49,050",info,16399,com.amazonaws.glue.ml.dataquality.dqdl.model.condition.number.NumberBasedCondition,Thread-10,78,,17.0 > 0.0? true,2025-09-10
1757511350377,"2025-09-10T13:35:50,377",info,17726,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint,dispatcher-CoarseGrainedScheduler,60,,"Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.35.6.172:57040) with ID 1,  ResourceProfileId 0",2025-09-10
1757511350379,"2025-09-10T13:35:50,379",info,17728,org.apache.spark.executor.ExecutorLogUrlHandler,dispatcher-CoarseGrainedScheduler,60,,Fail to renew executor log urls: some of required attributes are missing in app's event log.. Required: Set(CONTAINER_ID) / available: Set(). Falling back to show app's original log urls.,2025-09-10
1757511350382,"2025-09-10T13:35:50,381",info,17730,org.apache.spark.scheduler.cluster.glue.ExecutorEventListener,spark-listener-group-shared,60,,Got executor added event for 1 @ 1757511350379,2025-09-10
1757511350382,"2025-09-10T13:35:50,382",info,17731,org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator,spark-listener-group-shared,60,,connected executor 1,2025-09-10
1757511350456,"2025-09-10T13:35:50,456",info,17805,org.apache.spark.storage.BlockManagerMasterEndpoint,dispatcher-BlockManagerMaster,60,,"Registering block manager 172.35.6.172:40777 with 5.8 GiB RAM, BlockManagerId(1, 172.35.6.172, 40777, None)",2025-09-10
1757511350788,"2025-09-10T13:35:50,788",info,18137,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint,dispatcher-CoarseGrainedScheduler,60,,"Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.36.232.66:50532) with ID 2,  ResourceProfileId 0",2025-09-10
1757511350790,"2025-09-10T13:35:50,789",info,18138,org.apache.spark.scheduler.cluster.glue.ExecutorEventListener,spark-listener-group-shared,60,,Got executor added event for 2 @ 1757511350789,2025-09-10
1757511350790,"2025-09-10T13:35:50,789",info,18138,org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator,spark-listener-group-shared,60,,connected executor 2,2025-09-10
1757511350858,"2025-09-10T13:35:50,857",info,18206,org.apache.spark.storage.BlockManagerMasterEndpoint,dispatcher-BlockManagerMaster,60,,"Registering block manager 172.36.232.66:46411 with 5.8 GiB RAM, BlockManagerId(2, 172.36.232.66, 46411, None)",2025-09-10
1757511350919,"2025-09-10T13:35:50,918",info,18267,org.apache.spark.sql.execution.SQLExecution,Thread-10,60,,Generating and posting SparkListenerSQLExecutionObfuscatedInfo...,2025-09-10
1757511350922,"2025-09-10T13:35:50,921",info,18270,org.apache.spark.sql.execution.SQLExecution,Thread-10,60,,Posted SparkListenerSQLExecutionObfuscatedInfo in 3 ms,2025-09-10
1757511351676,"2025-09-10T13:35:51,676",info,19025,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint,dispatcher-CoarseGrainedScheduler,60,,"Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.34.197.149:44190) with ID 3,  ResourceProfileId 0",2025-09-10
1757511351678,"2025-09-10T13:35:51,677",info,19026,org.apache.spark.scheduler.cluster.glue.ExecutorEventListener,spark-listener-group-shared,60,,Got executor added event for 3 @ 1757511351677,2025-09-10
1757511351678,"2025-09-10T13:35:51,678",info,19027,org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator,spark-listener-group-shared,60,,connected executor 3,2025-09-10
1757511351757,"2025-09-10T13:35:51,756",info,19105,org.apache.spark.storage.BlockManagerMasterEndpoint,dispatcher-BlockManagerMaster,60,,"Registering block manager 172.34.197.149:46121 with 5.8 GiB RAM, BlockManagerId(3, 172.34.197.149, 46121, None)",2025-09-10
1757511351878,"2025-09-10T13:35:51,877",info,19226,org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator,Thread-10,60,,Code generated in 81.1153 ms,2025-09-10
1757511352037,"2025-09-10T13:35:52,036",info,19385,org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator,Thread-10,60,,Code generated in 39.991893 ms,2025-09-10
1757511352070,"2025-09-10T13:35:52,070",trace,19419,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511352173,"2025-09-10T13:35:52,172",info,19521,org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator,Thread-10,60,,Code generated in 34.492877 ms,2025-09-10
1757511352183,"2025-09-10T13:35:52,183",trace,19532,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511352186,"2025-09-10T13:35:52,185",info,19534,com.amazonaws.services.glue.GlueContext,Thread-10,917,,Glue secret manager integration: secretId is not provided.,2025-09-10
1757511352188,"2025-09-10T13:35:52,187",trace,19536,com.amazonaws.services.glue.GlueContext,Thread-10,1116,,datasink_s3 is used.,2025-09-10
1757511352267,"2025-09-10T13:35:52,267",info,19616,com.amazonaws.services.glue.GlueContext,Thread-10,1170,,The DataSink in action for the given format/connectionType (s3) is com.amazonaws.services.glue.sinks.HadoopDataSink,2025-09-10
1757511352438,"2025-09-10T13:35:52,437",info,19786,com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory,Thread-10,72,,"Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)",2025-09-10
1757511352442,"2025-09-10T13:35:52,441",info,19790,com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory,Thread-10,85,,Set initial getObject socket timeout to 2000 ms.,2025-09-10
1757511352461,"2025-09-10T13:35:52,460",info,19809,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_3 stored as values in memory (estimated size 24.0 B, free 5.8 GiB)",2025-09-10
1757511352463,"2025-09-10T13:35:52,463",info,19812,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_3_piece0 stored as bytes in memory (estimated size 89.0 B, actual size: 89.0 B, free 5.8 GiB)",2025-09-10
1757511352464,"2025-09-10T13:35:52,464",info,19813,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_3_piece0 in memory on 172.39.49.172:37387 (size: 89.0 B, free: 5.8 GiB)",2025-09-10
1757511352471,"2025-09-10T13:35:52,471",info,19820,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 3 from broadcast at HadoopDataSink.scala:207,2025-09-10
1757511352485,"2025-09-10T13:35:52,484",info,19833,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_4 stored as values in memory (estimated size 88.0 B, free 5.8 GiB)",2025-09-10
1757511352489,"2025-09-10T13:35:52,489",info,19838,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_4_piece0 stored as bytes in memory (estimated size 315.0 B, actual size: 315.0 B, free 5.8 GiB)",2025-09-10
1757511352490,"2025-09-10T13:35:52,490",info,19839,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_4_piece0 in memory on 172.39.49.172:37387 (size: 315.0 B, free: 5.8 GiB)",2025-09-10
1757511352492,"2025-09-10T13:35:52,491",info,19840,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 4 from broadcast at HadoopDataSink.scala:213,2025-09-10
1757511352542,"2025-09-10T13:35:52,541",info,19890,org.apache.spark.SparkContext,Thread-10,60,,Starting job: runJob at GlueParquetHadoopWriter.scala:176,2025-09-10
1757511352564,"2025-09-10T13:35:52,563",info,19912,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Got job 0 (runJob at GlueParquetHadoopWriter.scala:176) with 1 output partitions,2025-09-10
1757511352564,"2025-09-10T13:35:52,564",info,19913,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Final stage: ResultStage 0 (runJob at GlueParquetHadoopWriter.scala:176),2025-09-10
1757511352565,"2025-09-10T13:35:52,564",info,19913,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Parents of final stage: List(),2025-09-10
1757511352567,"2025-09-10T13:35:52,567",info,19916,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Missing parents: List(),2025-09-10
1757511352594,"2025-09-10T13:35:52,594",info,19943,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,"Submitting ResultStage 0 (MapPartitionsRDD[27] at filter at DynamicFrame.scala:165), which has no missing parents",2025-09-10
1757511352665,"2025-09-10T13:35:52,665",info,20014,org.apache.spark.storage.memory.MemoryStore,dag-scheduler-event-loop,60,,"Block broadcast_5 stored as values in memory (estimated size 225.7 KiB, free 5.8 GiB)",2025-09-10
1757511352671,"2025-09-10T13:35:52,671",info,20020,org.apache.spark.storage.memory.MemoryStore,dag-scheduler-event-loop,60,,"Block broadcast_5_piece0 stored as bytes in memory (estimated size 83.0 KiB, actual size: 83.0 KiB, free 5.8 GiB)",2025-09-10
1757511352672,"2025-09-10T13:35:52,672",info,20021,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_5_piece0 in memory on 172.39.49.172:37387 (size: 83.0 KiB, free: 5.8 GiB)",2025-09-10
1757511352674,"2025-09-10T13:35:52,674",info,20023,org.apache.spark.SparkContext,dag-scheduler-event-loop,60,,Created broadcast 5 from broadcast at DAGScheduler.scala:1664,2025-09-10
1757511352699,"2025-09-10T13:35:52,698",info,20047,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[27] at filter at DynamicFrame.scala:165) (first 15 tasks are for partitions Vector(0)),2025-09-10
1757511352701,"2025-09-10T13:35:52,700",info,20049,org.apache.spark.scheduler.TaskSchedulerImpl,dag-scheduler-event-loop,60,,Adding task set 0.0 with 1 tasks resource profile 0,2025-09-10
1757511352735,"2025-09-10T13:35:52,735",info,20084,org.apache.spark.scheduler.TaskSetManager,dispatcher-CoarseGrainedScheduler,60,,"Starting task 0.0 in stage 0.0 (TID 0) (172.36.232.66, executor 2, partition 0, ANY, 9866 bytes)",2025-09-10
1757511353018,"2025-09-10T13:35:53,018",info,20367,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_5_piece0 in memory on 172.36.232.66:46411 (size: 83.0 KiB, free: 5.8 GiB)",2025-09-10
1757511354324,"2025-09-10T13:35:54,324",info,21673,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_2_piece0 in memory on 172.36.232.66:46411 (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511365819,"2025-09-10T13:36:05,818",info,33167,org.apache.spark.scheduler.TaskSetManager,task-result-getter-0,60,,Finished task 0.0 in stage 0.0 (TID 0) in 13098 ms on 172.36.232.66 (executor 2) (1/1),2025-09-10
1757511365821,"2025-09-10T13:36:05,821",info,33170,org.apache.spark.scheduler.TaskSchedulerImpl,task-result-getter-0,60,,"Removed TaskSet 0.0, whose tasks have all completed, from pool",2025-09-10
1757511365825,"2025-09-10T13:36:05,825",info,33174,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,ResultStage 0 (runJob at GlueParquetHadoopWriter.scala:176) finished in 13.195 s,2025-09-10
1757511365835,"2025-09-10T13:36:05,835",info,33184,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Job 0 is finished. Cancelling potential speculative or zombie tasks for this job,2025-09-10
1757511365836,"2025-09-10T13:36:05,836",info,33185,org.apache.spark.scheduler.TaskSchedulerImpl,dag-scheduler-event-loop,60,,Killing all running tasks in stage 0: Stage finished,2025-09-10
1757511365839,"2025-09-10T13:36:05,838",info,33187,org.apache.spark.scheduler.DAGScheduler,Thread-10,60,,"Job 0 finished: runJob at GlueParquetHadoopWriter.scala:176, took 13.296707 s",2025-09-10
1757511365842,"2025-09-10T13:36:05,842",info,33191,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,265,,enableUpdateCatalog = false,2025-09-10
1757511365843,"2025-09-10T13:36:05,842",info,33191,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,266,,partitionKeys is empty - true,2025-09-10
1757511365844,"2025-09-10T13:36:05,843",info,33192,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,267,,"nameSpace: , table:",2025-09-10
1757511365846,"2025-09-10T13:36:05,846",trace,33195,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511365849,"2025-09-10T13:36:05,848",info,33197,com.amazonaws.services.glue.GlueContext,Thread-10,917,,Glue secret manager integration: secretId is not provided.,2025-09-10
1757511365850,"2025-09-10T13:36:05,849",trace,33198,com.amazonaws.services.glue.GlueContext,Thread-10,1116,,datasink_s3 is used.,2025-09-10
1757511365851,"2025-09-10T13:36:05,851",info,33200,com.amazonaws.services.glue.GlueContext,Thread-10,1170,,The DataSink in action for the given format/connectionType (s3) is com.amazonaws.services.glue.sinks.HadoopDataSink,2025-09-10
1757511365889,"2025-09-10T13:36:05,889",info,33238,com.amazonaws.services.glue.writers.AvroWriterUtil.AvroRecordBuilderUtil$,Thread-10,246,,Creating default Avro record builder for version 1.7.,2025-09-10
1757511365905,"2025-09-10T13:36:05,904",info,33253,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_6 stored as values in memory (estimated size 24.0 B, free 5.8 GiB)",2025-09-10
1757511365907,"2025-09-10T13:36:05,907",info,33256,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_6_piece0 stored as bytes in memory (estimated size 89.0 B, actual size: 89.0 B, free 5.8 GiB)",2025-09-10
1757511365908,"2025-09-10T13:36:05,907",info,33256,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_6_piece0 in memory on 172.39.49.172:37387 (size: 89.0 B, free: 5.8 GiB)",2025-09-10
1757511365910,"2025-09-10T13:36:05,909",info,33258,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 6 from broadcast at HadoopDataSink.scala:207,2025-09-10
1757511365911,"2025-09-10T13:36:05,911",info,33260,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_7 stored as values in memory (estimated size 88.0 B, free 5.8 GiB)",2025-09-10
1757511365913,"2025-09-10T13:36:05,912",info,33261,org.apache.spark.storage.memory.MemoryStore,Thread-10,60,,"Block broadcast_7_piece0 stored as bytes in memory (estimated size 315.0 B, actual size: 315.0 B, free 5.8 GiB)",2025-09-10
1757511365914,"2025-09-10T13:36:05,914",info,33263,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_7_piece0 in memory on 172.39.49.172:37387 (size: 315.0 B, free: 5.8 GiB)",2025-09-10
1757511365986,"2025-09-10T13:36:05,915",info,33264,org.apache.spark.SparkContext,Thread-10,60,,Created broadcast 7 from broadcast at HadoopDataSink.scala:213,2025-09-10
1757511365986,"2025-09-10T13:36:05,938",info,33287,org.apache.spark.SparkContext,Thread-10,60,,Starting job: runJob at HadoopWriters.scala:129,2025-09-10
1757511365986,"2025-09-10T13:36:05,940",info,33289,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Got job 1 (runJob at HadoopWriters.scala:129) with 1 output partitions,2025-09-10
1757511365986,"2025-09-10T13:36:05,942",info,33291,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Final stage: ResultStage 1 (runJob at HadoopWriters.scala:129),2025-09-10
1757511365986,"2025-09-10T13:36:05,943",info,33292,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Parents of final stage: List(),2025-09-10
1757511365986,"2025-09-10T13:36:05,943",info,33292,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Missing parents: List(),2025-09-10
1757511365986,"2025-09-10T13:36:05,944",info,33293,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,"Submitting ResultStage 1 (MapPartitionsRDD[29] at filter at DynamicFrame.scala:165), which has no missing parents",2025-09-10
1757511365987,"2025-09-10T13:36:05,980",info,33329,org.apache.spark.storage.memory.MemoryStore,dag-scheduler-event-loop,60,,"Block broadcast_8 stored as values in memory (estimated size 225.7 KiB, free 5.8 GiB)",2025-09-10
1757511365987,"2025-09-10T13:36:05,985",info,33334,org.apache.spark.storage.memory.MemoryStore,dag-scheduler-event-loop,60,,"Block broadcast_8_piece0 stored as bytes in memory (estimated size 83.1 KiB, actual size: 83.1 KiB, free 5.8 GiB)",2025-09-10
1757511365987,"2025-09-10T13:36:05,986",info,33335,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_8_piece0 in memory on 172.39.49.172:37387 (size: 83.1 KiB, free: 5.8 GiB)",2025-09-10
1757511365989,"2025-09-10T13:36:05,989",info,33338,org.apache.spark.SparkContext,dag-scheduler-event-loop,60,,Created broadcast 8 from broadcast at DAGScheduler.scala:1664,2025-09-10
1757511365991,"2025-09-10T13:36:05,991",info,33340,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[29] at filter at DynamicFrame.scala:165) (first 15 tasks are for partitions Vector(0)),2025-09-10
1757511365992,"2025-09-10T13:36:05,992",info,33341,org.apache.spark.scheduler.TaskSchedulerImpl,dag-scheduler-event-loop,60,,Adding task set 1.0 with 1 tasks resource profile 0,2025-09-10
1757511365994,"2025-09-10T13:36:05,994",info,33343,org.apache.spark.scheduler.TaskSetManager,dispatcher-CoarseGrainedScheduler,60,,"Starting task 0.0 in stage 1.0 (TID 1) (172.34.197.149, executor 3, partition 0, ANY, 9866 bytes)",2025-09-10
1757511366316,"2025-09-10T13:36:06,315",info,33664,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_8_piece0 in memory on 172.34.197.149:46121 (size: 83.1 KiB, free: 5.8 GiB)",2025-09-10
1757511366812,"2025-09-10T13:36:06,811",info,34160,com.amazonaws.services.glue.LogPusher,pool-6-thread-1,60,,uploading file:///var/log/spark/apps to s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/,2025-09-10
1757511367181,"2025-09-10T13:36:07,180",info,34529,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_3_piece0 on 172.39.49.172:37387 in memory (size: 89.0 B, free: 5.8 GiB)",2025-09-10
1757511367226,"2025-09-10T13:36:07,225",info,34574,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_5_piece0 on 172.39.49.172:37387 in memory (size: 83.0 KiB, free: 5.8 GiB)",2025-09-10
1757511367229,"2025-09-10T13:36:07,229",info,34578,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_5_piece0 on 172.36.232.66:46411 in memory (size: 83.0 KiB, free: 5.8 GiB)",2025-09-10
1757511367234,"2025-09-10T13:36:07,234",info,34583,com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream,pool-6-thread-1,429,,close closed:false s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/jr_abeec7a629dbdd0b2a85341d68fc44240c90423c19d39a89788cb1e38c7c826d.inprogress,2025-09-10
1757511367243,"2025-09-10T13:36:07,243",info,34592,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_4_piece0 on 172.39.49.172:37387 in memory (size: 315.0 B, free: 5.8 GiB)",2025-09-10
1757511367590,"2025-09-10T13:36:07,589",info,34938,com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream,pool-6-thread-1,429,,close closed:false s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/spark-application-1757511341101.inprogress,2025-09-10
1757511368240,"2025-09-10T13:36:08,240",info,35589,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Added broadcast_2_piece0 in memory on 172.34.197.149:46121 (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511372448,"2025-09-10T13:36:12,442",error,39791,org.apache.spark.metrics.sink.GlueCloudWatchReporter,metrics-coda-hale-metrics-cloud-watch-reporter-1-thread-1,346,,"Error reporting metrics to CloudWatch. The data in this CloudWatch API request may have been discarded, did not make it to CloudWatch.",2025-09-10
1757511383443,"2025-09-10T13:36:23,443",info,50792,org.apache.spark.scheduler.TaskSetManager,task-result-getter-1,60,,Finished task 0.0 in stage 1.0 (TID 1) in 17450 ms on 172.34.197.149 (executor 3) (1/1),2025-09-10
1757511383444,"2025-09-10T13:36:23,443",info,50792,org.apache.spark.scheduler.TaskSchedulerImpl,task-result-getter-1,60,,"Removed TaskSet 1.0, whose tasks have all completed, from pool",2025-09-10
1757511383445,"2025-09-10T13:36:23,444",info,50793,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,ResultStage 1 (runJob at HadoopWriters.scala:129) finished in 17.490 s,2025-09-10
1757511383445,"2025-09-10T13:36:23,445",info,50794,org.apache.spark.scheduler.DAGScheduler,dag-scheduler-event-loop,60,,Job 1 is finished. Cancelling potential speculative or zombie tasks for this job,2025-09-10
1757511383445,"2025-09-10T13:36:23,445",info,50794,org.apache.spark.scheduler.TaskSchedulerImpl,dag-scheduler-event-loop,60,,Killing all running tasks in stage 1: Stage finished,2025-09-10
1757511383447,"2025-09-10T13:36:23,446",info,50795,org.apache.spark.scheduler.DAGScheduler,Thread-10,60,,"Job 1 finished: runJob at HadoopWriters.scala:129, took 17.508275 s",2025-09-10
1757511383447,"2025-09-10T13:36:23,447",info,50796,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,265,,enableUpdateCatalog = false,2025-09-10
1757511383447,"2025-09-10T13:36:23,447",info,50796,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,266,,partitionKeys is empty - true,2025-09-10
1757511383448,"2025-09-10T13:36:23,447",info,50796,com.amazonaws.services.glue.sinks.HadoopDataSink,Thread-10,267,,"nameSpace: , table:",2025-09-10
1757511383449,"2025-09-10T13:36:23,448",trace,50797,com.amazonaws.services.glue.DynamicFrame,Thread-10,286,,DynamicFrame - create,2025-09-10
1757511383502,"2025-09-10T13:36:23,502",info,50851,com.amazonaws.services.glue.ProcessLauncher,main,60,,postprocessing,2025-09-10
1757511383502,"2025-09-10T13:36:23,502",info,50851,com.amazonaws.services.glue.ProcessLauncher,main,661,,Enhance failure reason and emit cloudwatch error metrics.,2025-09-10
1757511383504,"2025-09-10T13:36:23,503",debug,50852,com.amazonaws.services.glue.ProcessLauncher,main,666,,Original failureReason:,2025-09-10
1757511383504,"2025-09-10T13:36:23,504",debug,50853,com.amazonaws.services.glue.ProcessLauncher,main,667,,exceptionErrorMessage is:,2025-09-10
1757511383521,"2025-09-10T13:36:23,521",info,50870,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,34,,Emit job error metrics,2025-09-10
1757511383602,"2025-09-10T13:36:23,602",info,50951,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,47,,"Failed to emit error metric data: User: arn:aws:sts::718465053629:assumed-role/aws-glue-notebook/GlueJobRunnerSession is not authorized to perform: cloudwatch:PutMetricData because no identity-based policy allows the cloudwatch:PutMetricData action (Service: CloudWatch, Status Code: 403, Request ID: 5ae7528c-4e9e-49ea-b415-97e92f2330a1)",2025-09-10
1757511383603,"2025-09-10T13:36:23,602",info,50951,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,48,,Retry attempt 1/3,2025-09-10
1757511386740,"2025-09-10T13:36:26,740",info,54089,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_6_piece0 on 172.39.49.172:37387 in memory (size: 89.0 B, free: 5.8 GiB)",2025-09-10
1757511386746,"2025-09-10T13:36:26,746",info,54095,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_8_piece0 on 172.39.49.172:37387 in memory (size: 83.1 KiB, free: 5.8 GiB)",2025-09-10
1757511386752,"2025-09-10T13:36:26,752",info,54101,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_8_piece0 on 172.34.197.149:46121 in memory (size: 83.1 KiB, free: 5.8 GiB)",2025-09-10
1757511386764,"2025-09-10T13:36:26,764",info,54113,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_7_piece0 on 172.39.49.172:37387 in memory (size: 315.0 B, free: 5.8 GiB)",2025-09-10
1757511386780,"2025-09-10T13:36:26,779",info,54128,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_2_piece0 on 172.39.49.172:37387 in memory (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511386781,"2025-09-10T13:36:26,780",info,54129,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_2_piece0 on 172.36.232.66:46411 in memory (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511386784,"2025-09-10T13:36:26,783",info,54132,org.apache.spark.storage.BlockManagerInfo,dispatcher-BlockManagerMaster,60,,"Removed broadcast_2_piece0 on 172.34.197.149:46121 in memory (size: 41.8 KiB, free: 5.8 GiB)",2025-09-10
1757511387572,"2025-09-10T13:36:27,571",info,54920,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,34,,Emit job error metrics,2025-09-10
1757511387582,"2025-09-10T13:36:27,581",info,54930,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,47,,"Failed to emit error metric data: User: arn:aws:sts::718465053629:assumed-role/aws-glue-notebook/GlueJobRunnerSession is not authorized to perform: cloudwatch:PutMetricData because no identity-based policy allows the cloudwatch:PutMetricData action (Service: CloudWatch, Status Code: 403, Request ID: 27602b0e-aa57-4dd1-a63e-0b66695f4fca)",2025-09-10
1757511387582,"2025-09-10T13:36:27,582",info,54931,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,48,,Retry attempt 2/3,2025-09-10
1757511391521,"2025-09-10T13:36:31,521",info,58870,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,34,,Emit job error metrics,2025-09-10
1757511391532,"2025-09-10T13:36:31,531",info,58880,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,47,,"Failed to emit error metric data: User: arn:aws:sts::718465053629:assumed-role/aws-glue-notebook/GlueJobRunnerSession is not authorized to perform: cloudwatch:PutMetricData because no identity-based policy allows the cloudwatch:PutMetricData action (Service: CloudWatch, Status Code: 403, Request ID: 03b585d5-f354-4e1c-b1bc-bbb47161f725)",2025-09-10
1757511391532,"2025-09-10T13:36:31,532",info,58881,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,48,,Retry attempt 3/3,2025-09-10
1757511391532,"2025-09-10T13:36:31,532",warn,58881,com.amazonaws.services.glue.CloudWatchMetricsEmitter,main,52,,Max retries reached. Unable to emit error metric data.,2025-09-10
1757511391538,"2025-09-10T13:36:31,538",info,58887,com.amazonaws.services.glue.LogPusher,main,60,,stopping,2025-09-10
1757511391541,"2025-09-10T13:36:31,541",info,58890,org.apache.spark.SparkContext,shutdown-hook-0,60,,Invoking stop() from shutdown hook,2025-09-10
1757511391541,"2025-09-10T13:36:31,541",info,58890,org.apache.spark.SparkContext,shutdown-hook-0,60,,SparkContext is stopping with exitCode 0.,2025-09-10
1757511391545,"2025-09-10T13:36:31,545",info,58894,com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter,spark-listener-group-shared,70,,"Logs, events processed and insights are written to file /tmp/glue-exception-analysis-logs/spark-application-1757511341101",2025-09-10
1757511391547,"2025-09-10T13:36:31,546",info,58895,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend,shutdown-hook-0,60,,Stopping JES Scheduler Backend.,2025-09-10
1757511391548,"2025-09-10T13:36:31,547",info,58896,glue.ch.cern.sparkmeasure.FlightRecorderStageMetrics,spark-listener-group-shared,43,,"Spark application ended, timestamp = 1757511391542",2025-09-10
1757511391548,"2025-09-10T13:36:31,548",warn,58897,glue.ch.cern.sparkmeasure.FlightRecorderStageMetrics,spark-listener-group-shared,57,,Writing Stage Metrics data serialized as json to /tmp/stageMetrics_flightRecorder,2025-09-10
1757511391548,"2025-09-10T13:36:31,548",info,58897,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend,shutdown-hook-0,60,,Shutting down all executors,2025-09-10
1757511391549,"2025-09-10T13:36:31,548",info,58897,org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint,dispatcher-CoarseGrainedScheduler,60,,Asking each executor to shut down,2025-09-10
1757511391649,"2025-09-10T13:36:31,649",info,58998,glue.ch.cern.sparkmeasure.FlightRecorderTaskMetrics,spark-listener-group-shared,102,,"Spark application ended, timestamp = 1757511391542",2025-09-10
1757511391658,"2025-09-10T13:36:31,657",info,59006,glue.ch.cern.sparkmeasure.FlightRecorderTaskMetrics,spark-listener-group-shared,116,,Writing Task Metrics data serialized as json to /tmp/taskMetrics_flightRecorder,2025-09-10
1757511391737,"2025-09-10T13:36:31,737",info,59086,org.apache.spark.MapOutputTrackerMasterEndpoint,dispatcher-event-loop-3,60,,MapOutputTrackerMasterEndpoint stopped!,2025-09-10
1757511391752,"2025-09-10T13:36:31,751",info,59100,org.apache.spark.storage.memory.MemoryStore,shutdown-hook-0,60,,MemoryStore cleared,2025-09-10
1757511391753,"2025-09-10T13:36:31,753",info,59102,org.apache.spark.storage.BlockManager,shutdown-hook-0,60,,BlockManager stopped,2025-09-10
1757511391759,"2025-09-10T13:36:31,759",info,59108,org.apache.spark.storage.BlockManagerMaster,shutdown-hook-0,60,,BlockManagerMaster stopped,2025-09-10
1757511391825,"2025-09-10T13:36:31,823",error,59172,org.apache.spark.metrics.sink.GlueCloudWatchReporter,shutdown-hook-0,346,,"Error reporting metrics to CloudWatch. The data in this CloudWatch API request may have been discarded, did not make it to CloudWatch.",2025-09-10
1757511391857,"2025-09-10T13:36:31,857",info,59206,org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint,dispatcher-event-loop-3,60,,OutputCommitCoordinator stopped!,2025-09-10
1757511391909,"2025-09-10T13:36:31,908",info,59257,org.apache.spark.SparkContext,shutdown-hook-0,60,,Successfully stopped SparkContext,2025-09-10
1757511391910,"2025-09-10T13:36:31,910",info,59259,com.amazonaws.services.glue.LogPusher,shutdown-hook-0,60,,uploading file:///var/log/spark/apps to s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/,2025-09-10
1757511391988,"2025-09-10T13:36:31,987",info,59336,com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream,shutdown-hook-0,429,,close closed:false s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/jr_abeec7a629dbdd0b2a85341d68fc44240c90423c19d39a89788cb1e38c7c826d,2025-09-10
1757511392086,"2025-09-10T13:36:32,085",info,59434,com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream,shutdown-hook-0,429,,close closed:false s3://aws-glue-assets-718465053629-us-east-1/sparkHistoryLogs/spark-application-1757511341101,2025-09-10
1757511392147,"2025-09-10T13:36:32,147",info,59496,org.apache.spark.util.ShutdownHookManager,shutdown-hook-0,60,,Shutdown hook called,2025-09-10
1757511392148,"2025-09-10T13:36:32,147",info,59496,org.apache.spark.util.ShutdownHookManager,shutdown-hook-0,60,,Deleting directory /tmp/spark-1341c081-3290-40d6-b503-fb7367c2e58b/pyspark-5f4d84e5-1a2a-4ab6-b5d5-8ff70ee2223d,2025-09-10
1757511392153,"2025-09-10T13:36:32,153",info,59502,org.apache.spark.util.ShutdownHookManager,shutdown-hook-0,60,,Deleting directory /tmp/spark-0d8aa955-c5b7-4d3a-9a7e-8692efe0179a,2025-09-10
1757511392160,"2025-09-10T13:36:32,159",info,59508,org.apache.spark.util.ShutdownHookManager,shutdown-hook-0,60,,Deleting directory /tmp/spark-1341c081-3290-40d6-b503-fb7367c2e58b,2025-09-10
